# ML Systems Learning Plan

ML Systems focuses on the systems level operations on how machine learning platforms are built. Its a combination of machine learning and computer systems programming. This learning path in  particular focuses on machine learning systems for efficient inference and distributed training in grid and federated environments. 

This learning plan breaks down topics into the following categories:
- Deep Learning : This focuses on the fundametals of deep learning algorithms and techniques. 
- Computer Systems: This focuses on topics in computer systems that are relevent to MLSys
- High Performance Computing: This focuses on the building and programming of high performance computing systems 
- Deep Learning Hardware and Compilers: This focuses on accelators and GPU programming specifically for deep learning algorithms and also includes knowledge on mapping the models to the deep learning hardware (compilation)
- Inference Systems: This focuses on the design and implementation of software systems for serving deep learning applications
- Distributed Systems: This focuses on basic ideas in distributed systems particularly around distributed training and federated learning
- Grid Computing: This focuses on the design and implementation of grid computing systems particularly for desktop grid computing environments

Each of these topic categories is broken down into fundamentals interemediate and advanced, which is used for structuring the learning plan.

#### Courses

* Deep Learning Core
 - Deep Learning Specialization [link](https://www.coursera.org/specializations/deep-learning)
   - Course 1, 2 & 3 – Fundamentals
   - Course 4 & 5 - Intermediate

* Computer Systems
 - Real-Time Embedded Systems Specialization [link](https://www.coursera.org/specializations/real-time-embedded-systems)
   - Course 1 and 2 – Intermediate
   - Course 3 and 4 – Advanced

* Deep Learning Hardware and Compilers [link](https://www.coursera.org/specializations/gpu-programming)
 - GPU Programming Specialization
   - Course 1 & 2 – Fundamentals
   - Course 3 & 4 - Intermediate
  
* Grid Computing
  - Cloud Computing Specialization [link](https://www.coursera.org/specializations/cloud-computing)
    - Course 1 & Course 2 – Fundamentals
    - Course 3 & Course 4 – Intermediate
    - Course 5 & Course 6 – Advanced
  
* Distributed Systems
  - Computer Communications Specialization[link](https://www.coursera.org/specializations/computer-communications)
        - Course 1 & 2 – Fundamentals
        - Course 3 & 4 – Intermediate

## BOOKS

### Deep Learning Core

1.  Dive into Deep Learning [link](https://d2l.ai)
2.  Math and Architectures of Deep Learning [link](https://www.amazon.co.uk/Math-Architectures-Learning-Krishnendu-Chaudhury/dp/1617296481)
3.  Deep Learning Architectures: A mathematical Approach [link](https://www.amazon.co.uk/Deep-Learning-Architectures-Mathematical-Approach/dp/3030367207)
4.  Neural Networks from Scratch Python [link](https://nnfs.io)

### Computer Systems

1.  Computer Systems: A programmers perspective [link](http://csapp.cs.cmu.edu)
2.  Engineering a Compiler [link](https://www.amazon.co.uk/Engineering-Compiler-Keith-Cooper/dp/155860698X)

### Deep Learning Hardware and Compilers

1.  Deep Learning Systems Algorithms Compilers and Processors [link](https://link.springer.com/book/10.1007/978-3-031-01769-8)
2.  Machine Learning Compilation [link](https://mlc.ai)
3.  Data Orchestration in Deep Learning Accelerators [link](https://link.springer.com/book/10.1007/978-3-031-01767-4)
4.  Programming Massively Parallel Processors [link](https://www.amazon.co.uk/Programming-Massively-Parallel-Processors-Hands/dp/0123814723)
5.  Hardware Accelerator Systems for AI and ML [link](https://www.amazon.co.uk/Hardware-Accelerator-Artificial-Intelligence-Learning-ebook/dp/B091F2NBQK)
6.  CUDA by Example [link](https://www.amazon.co.uk/CUDA-Example-Introduction-General-Purpose-Programming/dp/0131387685)

### High Performance Computing

1.  HPC Book 1 [link](https://theartofhpc.com/istc.html)
2.  HPC Book 2 [link](https://theartofhpc.com/pcse.html)
3.  HPC Book 3 [link](https://github.com/VictorEijkhout/TheArtofHPC_pdfs/blob/main/vol3/EijkhoutIntroSciProgramming-book.pdf)
4.  HPC Projects Book [link](https://github.com/VictorEijkhout/TheArtofHPC_pdfs/blob/main/vol3/EijkhoutProgrammingProjects-book.pdf)
5.  Parallel and High-Performance Computing [link](https://www.amazon.co.uk/Parallel-Performance-Computing-Robert-Robey/dp/1617296465)

### Inference Systems

1.  Designing Deep Learning Systems a Software Engineers Guide [link](https://www.amazon.com/Engineering-Deep-Learning-Systems-Wang/dp/1633439860)
2.  Efficient Processing of Deep Neural Networks [link](https://link.springer.com/book/10.1007/978-3-031-01766-7)
3.  Awesome LLM Inference [link](https://github.com/DefTruth/Awesome-LLM-Inference)

### Grid Computing

1.  Desktop Grid Computing [link](https://www.amazon.co.uk/Desktop-Computing-Numerical-Analysis-Scientific-ebook/dp/B00OD4G81O)

### Distributed Systems

1.  Distributed Machine Learning Patterns [link](https://www.amazon.co.uk/Distributed-Machine-Learning-Patterns-Yuan/dp/1617299022)
2.  Distributed Machine Learning and Gradient Optimization [link](https://link.springer.com/content/pdf/10.1007/978-981-16-3420-8.pdf)
3.  Federated Learning Systems Towards Next-Generation AI [link](https://www.amazon.co.uk/Federated-Learning-Systems-Next-Generation-Computational/dp/3030706036)
4.  Awesome LLM Systems Papers [link](https://github.com/AmadeusChan/Awesome-LLM-System-Papers)


## Batches

The batches define sets of topics that should be studied together, each batch builds on the foundations of the previous batch. The batches are recommended to completed in 14 weeks (~ 3 months). The batches include a combination of content in the books mentioned above, research papers and coursera courses. doing projects at the end fo every batch is recommended.

### Batch 1:

*   **Deep Learning Fundamentals**
    *   Dive Into Deep Learning Chapter 1 - Chapter 7
    *   Math and Architectures of Deep Learning Chapter 1 - Chapter 11
    *   Neural Networks from Scratch Chapter 1 - Chapter 11
    *   DL Architectures: Math. Approach Chapter 1 - Chapter 6
    *   Deep Learning Specialization Course 1, 2, 3
*   **Computer Systems Fundamentals**
    *   Computer Systems Part 1: Chapter 1 - Chapter 6
*   **Deep Learning Intermediate**
    *   Dive Into Deep Learning Chapter 8 - Chapter 14
    *   Math and Architectures of Deep Learning Chapter 12 - Chapter 14
    *   Neural Networks from Scratch Chapter 12 - Chapter 22
    *   DL Architectures: Math. Approach Chapter 7 - Chapter 10
    *   Deep Learning Specialization Course 4, 5

### Batch 2:

*   **Computer Systems Intermediate**
    *   Computer Systems Part 2: Chapter 7 - Chapter 9
    *   Engineering a Compiler Chapter 1 - Chapter 6
    *   Real-Time Embedded Systems Course 1 & 2
*   **High Performance Computing Fundamentals**
    *   HPC Book 1 : Part 1 Chapter 1 - Chapter 7
    *   Parallel and High Performance Computing Part 1 (Chapter 1 - Chapter 5 ) & Part 2 (Chapter 6 - Chpater 8)
    *   HPC Book 3 Chapter 1 - Chapter 6
*   **High Performance Computing Intermediate**
    *   HPC Book 1 : Part 2 Chapter 8 - Chapter 13
    *   HPC Book 2 : Part 1 Chapter 1 - Chapter 16

### Batch 3:

*   **Deep Learning Advanced**
    *   Dive into Deep Learning Chpater 14 - Chapter 21
    *   Deep Learning Architectures a Math. Appr Chpater 11 - Chapter 20
*   **Deep Learning Hardware and Compilers Fundamentals**
    *   Data Orchestration in Deep Learning Accelerators Chapter 1 - Chapter 3
    *   CUDA by Example Chapter 1 - Chapter 6
    *   Programming Massively Parallel Processors Chapter 1 - Chapter 7
    *   Hardware Accelerators Systems for AI and ML Chapter 1 - Chapter 4
    *   GPU Programming Specialization Course 1 & 2
*   **Inference Systems Fundamentals**
    *   Designing Deep Learning Systems Chapter 1 - Chapter 5
    *   Efficient Processing of Deep Neural Networks Part 1 (Chapter 1 - Chapter 2)
    *   Efficient LLM Inference on CPUs (Shen et al., 2023)
    *   PagedAttention: Efficient Memory Management for Large Language Model Serving with PagedAttention (Kwon et al., 2023)
    *   TurboTransformers: An Efficient GPU Serving System For Transformer Models (PPoPP'21)
    *   FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU (ICML'23)
    *   PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU (preprint'23)

### Batch 4:

*   **Computer Systems Advanced**
    *   Computer Systems Part 3 Chapter 10 - Chapter 12
    *   Engineering a Compiler Chapter 7 - Chapter 13
    *   Real-Time Embedded Systems Course 3 & 4
*   **Deep Learning Hardware and Compilers Intermediate**
    *   Machine Lerning Compilation Chapter 1 - Chapter 7
    *   Data Orchestration in DL Accelerators Chapter 4 - Chapter 5
    *   CUDA by Example Chapter 7 - Chapter 12
    *   DL Sys. Algorithms Compilers and Processors Chapter 1 - Chapter 5
    *   Programming Massively Parallel Processors Chapter 8 - Chapter 14
    *   Hardware Accelerators Systems for AI and ML Chpater 5 - Chapter 8
    *   GPU Programming Specialization Course 3 & 4
*   **Inference Systems Intermediate**
    *   Designing Deep Learning Systems Chapter 1 - Chapter 5
    *   Efficeint Processing of Deep Neural Networks Part 2 (Chapter 3 - Chapter 6)
    *   FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (Dao et al., 2022)
    *   FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU (Sheng et al., 2023)
    *   ROFORMER: Enhanced Transformer with Rotary Position Embedding (Su et al., 2021)
    *   SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models (Xiao et al., 2023)
    *   ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers (Yao et al., 2022)
    *   Orca: A Distributed Serving System for Transformer-Based Generative Models (OSDI'22)
    *   DeepSpeed-inference: enabling efficient inference of transformer models at unprecedented scale (SC'22)
    *   EnergeonAI: An Inference System for 10-100 Billion Parameter Transformer Models (arXiv'22)
    *   Fast Distributed Inference Serving for Large Language Models (arXiv'23):
    *   LLM in a flash: Efficient Large Language Model Inference with Limited Memory (arXiv'23)

### Batch 5:

*   **Deep Learning Hardware and Compilers Advanced**
    *   Deep Learning Systems Algorithms Compilers and Processors Chapter 6 - Chapter 10
    *   Data Orcestration in Deep Learning Accelerators Chapter 6 - Chpater 8
    *   Programming Massively Parrallel Processors Chapter 15 - Chapter 21
    *   Hardware Accelerator Systems for AI and ML Chapter 9 - Chapter 11
*   **Inference Systems Advanced**
    *   Efficiient Processing of Deep Neural Networks Part 3 (Chapter 7 - Chapter 10)
    *   AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration (Lin et al., 2023)
    *   ByteTransformer: A High-Performance Transformer Boosted for Variable-Length Inputs (Zhai et al., 2023)
    *   FLASHDECODING++: FASTER LARGE LANGUAGE MODEL INFERENCE ON GPUS (Hong et al., 2023)
    *   FP8-LM: Training FP8 Large Language Models (Peng et al., 2023)
    *   FP8 FORMATS FOR DEEP LEARNING (Micikevicius et al., 2022)
    *   LibShalom: Optimizing Small and Irregular-shaped Matrix Multiplications on ARMv8 Multi-Cores (Yang et al., 2021)
    *   LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale (Dettmers et al., 2022)
    *   Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism (Shoeybi et al., 2020)
    *   NVIDIA Tensor Core Programmability, Performance & Precision (Markidis et al., 2018)
    *   Online normalizer calculation for softmax (Milakov and Gimelshein, 2018)
    *   Orca: A Distributed Serving System for Transformer-Based Generative Models (Yu et al., 2022)
    *   Reducing shared memory footprint to leverage high throughput on Tensor Cores and its flexible API extension library (Ootomo and Yokota, 2023)
    *   SHEARED LLAMA: ACCELERATING LANGUAGE MODEL PRE-TRAINING VIA STRUCTURED PRUNING (Xia et al., 2023)
    *   SpecInfer: Accelerating Generative Large Language Model Serving with Speculative Inference and Token Tree Verification (Miao et al., 2023)
    *   SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression (Dettmers et al., 2023)
    *   StreamLLM: EFFICIENT STREAMING LANGUAGE MODELS WITH ATTENTION SINKS (Xiao et al., 2023)
    *   WINT8: Who Says Elephants Can\u2019t Run: Bringing Large Scale MoE Models into Cloud Scale Production (Kim et al., 2022)
    *   ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats (Wu et al., 2023)
    *   Efficient Memory Management for Large Language Model Serving with PagedAttention (SOSP'23)
    *   EdgeMoE: Fast On-Device Inference of MoE-based Large Language Models (arXiv'23)
*  **Distributed Systems Fundamentals**
    *   Distributed Machine Learning Chapter 1 – Chapter 3 
    *   Distributed Machine Learning Systems Part 1 (Chapter 1) & Part 2 (Chapter 2 – Chapter 6)


### Batch 6:

*   **High Performance Computing Advanced**
    *   HPC Book 2 Chapter 17 - Chapter 51
    *   HPC Book 3 Chapter 13 - Chapter 18
*   **Distributed Systems Intermediate**
    *   Distributed Machine Learing Chapter 4 - Chapter 5
    *   Federated Learning Systems Chapter 5 - Chapter 8
    *   Distributed Machine Learning Ssytems Chapter 1 - Chapter 4
    *   CRAMMING: Training a Language Model on a Single GPU in One Day (arXiv'22)
    *   PETALS: Collaborative Inference and Fine-tuning of Large Models (NeurIPS'22 Workshop WBRC)
    *   Easy and Efficient Transformer: Scalable Inference Solution For large NLP model (arXiv'22)
    *   Efficient GPU Spatial-Temporal Multitasking (TPDS'14)
*   **Grid Computing Fundamentals**
    *   Desktop Grid Computing Chapter 1 - Chapter 5
    *   Cloud Computing Specialization Course 1 & 2

### Batch 7:

*   **Distributed Systems Advanced**
    *   Federated Learning Systems Chapter 5 - Chapter 8
    *   Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity (JMLR'21)
    *   Scalable and Efficient MoE Training for Multitask Multilingual Models (arXiv'21)
    *   DeepSpeed-MOE: Advancing Mixture of Experts Inference and Training to Power Next-Generation AI Scale (ICML'22
    *   ZeRO: Memory optimizations Toward Training Trillion Parameter Models (SC'20)
    *   Megatron-lm: Training multi-billion parameter language models using model parallelism (arXiv'20)
    *   Microsecond-scale Preemption for Concurrent GPU-accelerated DNN Inferences (OSDI'22)
*   **Grid Computing Intermediate**
    *   Desktop Grid Computing Chapter 6 - Chapter 10
    *   Cloud Computing Specialization Course 3 & 4
*   **Grid Computing Advanced**
    *   Desktop Grid Computing Chapter 11 - Chapter 15
    *   Cloud Computing Specialization Course 5 & 6

